{"cells":[{"cell_type":"code","source":["!pip install -U spacy[cuda]"],"metadata":{"id":"6eDMuKJua1Om"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"id":"vfP9rAYWbDCO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install flair"],"metadata":{"id":"e0c71geibNDo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"xHT-ynRqay-v","executionInfo":{"status":"ok","timestamp":1703096006609,"user_tz":-60,"elapsed":1648,"user":{"displayName":"Daria Stetsenko","userId":"08016856777783109624"}}},"outputs":[],"source":["import os\n","import re\n","import json\n","import nltk"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-7u4avB4bA94","executionInfo":{"status":"ok","timestamp":1703096038544,"user_tz":-60,"elapsed":29580,"user":{"displayName":"Daria Stetsenko","userId":"08016856777783109624"}},"outputId":"230530bc-5082-495c-9ead-72e56021b788"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6sgkPOtkay-1","executionInfo":{"status":"ok","timestamp":1703096056979,"user_tz":-60,"elapsed":10154,"user":{"displayName":"Daria Stetsenko","userId":"08016856777783109624"}},"outputId":"8fa4dbde-3416-473f-8f3d-8cf37630455d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}],"source":["import spacy\n","spacy.require_gpu()"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"aK0pWKhway-3","executionInfo":{"status":"ok","timestamp":1703096067633,"user_tz":-60,"elapsed":903,"user":{"displayName":"Daria Stetsenko","userId":"08016856777783109624"}}},"outputs":[],"source":["def load_book(book_path):\n","    \"\"\"\n","    Function to load a book from a text file\n","    :param book_path: Path to the book text file\n","    :return: The book text as a string\n","    \"\"\"\n","\n","    with open(book_path, 'r', encoding='utf-8') as book_file:\n","        book_text = book_file.read()\n","    return book_text"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"KRM2-QMWay-8","executionInfo":{"status":"ok","timestamp":1703096070780,"user_tz":-60,"elapsed":13,"user":{"displayName":"Daria Stetsenko","userId":"08016856777783109624"}}},"outputs":[],"source":["def extract_chapter_numbers(text):\n","    match = re.search(r'\\b([IVXLCDM]+|\\d+(\\.\\d+)?)\\.', text)\n","    chaper_numbers = []\n","    if match:\n","        return match.group(1)\n","    return None"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"7jenc4WWay-3","executionInfo":{"status":"ok","timestamp":1703096076689,"user_tz":-60,"elapsed":394,"user":{"displayName":"Daria Stetsenko","userId":"08016856777783109624"}}},"outputs":[],"source":["CHAPTERS = [\"Chapter\", \"CHAPTER\", \"ACT\"]\n","\n","def split_book_by_chapter(cleaned_text, spacy_model):\n","    \"\"\"\n","    Split the book into chapters\n","    :param cleaned_text: the text of the book with the headers and footers removed\n","    :return: a list of chapters\n","    \"\"\"\n","    add_chapter = []\n","\n","\n","    for title in CHAPTERS:\n","        cleaned_text = cleaned_text.replace(title, \"Chapter \")\n","    chapters = re.split(r'\\bChapter\\b', cleaned_text)\n","\n","    for chapter in chapters:\n","      if len(chapter) > 1000:\n","        number = extract_chapter_numbers(chapter)\n","        add_chapter.append((number, spacy_model(chapter[2:])))\n","\n","    return add_chapter"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"5LMFMQXFay-4","executionInfo":{"status":"ok","timestamp":1703096085203,"user_tz":-60,"elapsed":490,"user":{"displayName":"Daria Stetsenko","userId":"08016856777783109624"}}},"outputs":[],"source":["def perform_ner(text):\n","    \"\"\"\n","    Function to perform named entity recognition on a text\n","    :param text: The text to perform NER on\n","    :param spacy_model: The spaCy model to use for NER\n","    :return: A list of named entities\n","    \"\"\"\n","\n","    entities = []\n","\n","    for ent in text.ents:\n","        if ent.label_ == \"PERSON\":\n","            entities.append(ent.text)\n","\n","    return entities"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"K1liAkeEay-7","executionInfo":{"status":"ok","timestamp":1703096091065,"user_tz":-60,"elapsed":18,"user":{"displayName":"Daria Stetsenko","userId":"08016856777783109624"}}},"outputs":[],"source":["def count_entities(entities:list)->list:\n","    \"\"\"\n","    Counts the number of times each entity appears in the text\n","    :param text:\n","    :param words: list of words to count\n","    :return: dictionary with words as keys and number of times they appear in the text as values\n","    \"\"\"\n","    set_entities = set(entities)\n","    entities_tuples = {}\n","\n","    for word in set_entities:\n","        count = entities.count(word)\n","        if count > 5:\n","            entities_tuples[word] = count\n","\n","    return entities_tuples"]},{"cell_type":"code","source":["from flair.models import TextClassifier\n","from flair.data import Sentence"],"metadata":{"id":"3KozdPsn4DY4","executionInfo":{"status":"ok","timestamp":1703096096716,"user_tz":-60,"elapsed":1914,"user":{"displayName":"Daria Stetsenko","userId":"08016856777783109624"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def extract_sentiment_scores(sentence):\n","  sentiment_words = []\n","  classifier = TextClassifier.load('sentiment-fast')\n","  sentence_sentiment = Sentence(sentence)\n","  classifier.predict(sentence_sentiment)\n","\n","  if sentence_sentiment.labels:\n","    sentiment_score = sentence_sentiment.labels[0].score\n","    for token in sentence_sentiment.tokens:\n","      sentiment_words.append((token.text, sentiment_score))\n","  return sentiment_words"],"metadata":{"id":"k8kFKwFP4kBd","executionInfo":{"status":"ok","timestamp":1703096100250,"user_tz":-60,"elapsed":397,"user":{"displayName":"Daria Stetsenko","userId":"08016856777783109624"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def character_sentiment_per_chapter(chapter, entities):\n","    sentiment_words = {}\n","\n","    for char in entities:\n","        if char not in sentiment_words:\n","            sentiment_words[char] = []\n","\n","        weighted_sentiment = 0\n","\n","        for sent in chapter.sents:\n","          if char in sent.text and any(token for token in sent if token.pos_ in [\"ADJ\", \"ADV\"]):\n","            sentiment_words[char] = {\"sentiment_per_chapter\":[],\n","                                     \"context\": []}\n","            sentiment_words[char][\"context\"].append(sent.text)\n","\n","            char_idx = sent.text.index(char)\n","\n","            sentiment_scores = extract_sentiment_scores(sent.text)\n","\n","            for token in sent:\n","                for word in sentiment_scores:\n","                  if token.pos_ in [\"ADJ\", \"ADV\"] and token.text == word[0]:\n","                    sentiment_score = word[1]\n","                    sentiment_idx = abs(char_idx - token.i)\n","\n","                    try:\n","                        weighted_sentiment += sentiment_score / sentiment_idx\n","                    except ZeroDivisionError:\n","                        weighted_sentiment += 0.0\n","        sentiment_words[char][\"sentiment_per_chapter\"].append(weighted_sentiment)\n","\n","    return sentiment_words"],"metadata":{"id":"ovElLnq3mRxH","executionInfo":{"status":"ok","timestamp":1703096104994,"user_tz":-60,"elapsed":14,"user":{"displayName":"Daria Stetsenko","userId":"08016856777783109624"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["!python -m spacy download en_core_web_lg"],"metadata":{"id":"WdbGOOFgcsTK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","def character_sentiment_per_chapter(chapter, entities):\n","    sentiment_words = {}\n","\n","    for char in tqdm(entities, desc=\"Processing Entities\"):\n","        if char not in sentiment_words:\n","            sentiment_words[char] = {\"sentiment_per_chapter\": [], \"context\": []}\n","\n","        weighted_sentiment = 0\n","\n","        for sent in chapter.sents:\n","            if char in sent.text and any(token for token in sent if token.pos_ in [\"ADJ\", \"ADV\"]):\n","                sentiment_words[char][\"context\"].append(sent.text.replace(\"\\n\", \" \"))\n","\n","                char_idx = sent.text.index(char)\n","\n","                sentiment_scores = extract_sentiment_scores(sent.text)\n","\n","                for token in sent:\n","                    for word in sentiment_scores:\n","                        if token.pos_ in [\"ADJ\", \"ADV\"] and token.text == word[0]:\n","                            sentiment_score = word[1]\n","                            sentiment_idx = abs(char_idx - token.i)\n","\n","                            try:\n","                                weighted_sentiment += sentiment_score / sentiment_idx\n","                            except ZeroDivisionError:\n","                                weighted_sentiment += 0.0\n","\n","        sentiment_words[char][\"sentiment_per_chapter\"].append(weighted_sentiment)\n","        print(sentiment_words)\n","\n","\n","    return sentiment_words\n","\n"],"metadata":{"id":"g-FNnhaepcLy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def restructured_data(data):\n","    restructured_data = {}\n","\n","    for chapter, characters in data.items():\n","        for name, details in characters:\n","            if name not in restructured_data:\n","                restructured_data[name] = {\n","                    \"name\": name,\n","                    \"sentiments\": [],\n","                    \"aggregated_sentiment\": 0.0,\n","                    \"context\": []\n","                }\n","\n","            # Append sentiments and context\n","            restructured_data[name][\"sentiments\"].extend(\n","                [(chapter, sentiment) for sentiment in details[\"sentiment_per_chapter\"]]\n","            )\n","            restructured_data[name][\"context\"].extend(details[\"context\"])\n","\n","    # Calculate aggregated sentiment for each character\n","    for name, details in restructured_data.items():\n","        if details[\"sentiments\"]:\n","            details[\"aggregated_sentiment\"] = sum(sentiment[1] for sentiment in details[\"sentiments\"])\n","\n","    return list(restructured_data.values())"],"metadata":{"id":"zWrewJb-65-B","executionInfo":{"status":"ok","timestamp":1703104400453,"user_tz":-60,"elapsed":1560,"user":{"displayName":"Daria Stetsenko","userId":"08016856777783109624"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"id":"0_afRQHnay--","executionInfo":{"status":"ok","timestamp":1703096155398,"user_tz":-60,"elapsed":7750,"user":{"displayName":"Daria Stetsenko","userId":"08016856777783109624"}}},"outputs":[],"source":["def main():\n","    if len(sys.argv) < 3:\n","        print(\"Usage: python3 Part2.py <path_to_the_book> <path_to_store_json>\")\n","        sys.exit(1)\n","nlp = spacy.load(\"en_core_web_lg\")\n","\n","book_path = \"/content/drive/MyDrive/Alice_in_wonderland_clean.txt\"\n","book_text = load_book(book_path)\n","book_title = os.path.basename(book_path).replace(\"_clean.txt\", \"\")\n","\n","\n","chapters = split_book_by_chapter(book_text, nlp)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"zlRVMM-Fay-_","executionInfo":{"status":"ok","timestamp":1703096213560,"user_tz":-60,"elapsed":890,"user":{"displayName":"Daria Stetsenko","userId":"08016856777783109624"}}},"outputs":[],"source":["get_entities = [perform_ner(chapter[1]) for chapter in chapters]\n","list_of_characters = [char for lst in get_entities for char in lst]\n","\n","main_characters = list(count_entities(list_of_characters).keys())"]},{"cell_type":"code","source":["scores = {}\n","\n","for chapter in tqdm(chapters, desc=\"Processing Chapter\"):\n","    if chapter[0] not in scores:\n","        scores[chapter[0]] = []\n","\n","    get_sentiment = character_sentiment_per_chapter(chapter[1], main_characters)\n","    scores[chapter[0]].extend(get_sentiment.items())"],"metadata":{"id":"0MgIPcnM5W9a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = json.dumps(restructured_data(scores), ensure_ascii=False, indent=2)"],"metadata":{"id":"5mCLLMnlyIo1","executionInfo":{"status":"ok","timestamp":1703104800226,"user_tz":-60,"elapsed":830,"user":{"displayName":"Daria Stetsenko","userId":"08016856777783109624"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["file_name = f\"{book_title}_Sentiment.json\"\n","output_file_path = os.path.join(directory_path, file_name)\n","with open(\"/content/drive/MyDrive/results.json\", \"w\", encoding=\"utf-8\") as json_file:\n","    json_file.write(result)"],"metadata":{"id":"t19TsXXK9322","executionInfo":{"status":"ok","timestamp":1703104802017,"user_tz":-60,"elapsed":18,"user":{"displayName":"Daria Stetsenko","userId":"08016856777783109624"}}},"execution_count":84,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mgQhK7lc2ZLK"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}